{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89dbc56-491a-438b-bf99-89cb3eb1dbdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import argparse\n",
    "from collections import OrderedDict\n",
    "import datetime\n",
    "import gc\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "from numbers import Number\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 1500\n",
    "pd.options.display.max_columns = 200\n",
    "pd.options.display.width = 1000\n",
    "pd.set_option('max_colwidth', 400)\n",
    "import pdb\n",
    "import pickle\n",
    "import pprint as pp\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from deepsnap.batch import Batch as deepsnap_Batch\n",
    "import xarray as xr\n",
    "import math\n",
    "import dolfin\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..'))\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..', '..'))\n",
    "from lamp.argparser import arg_parse\n",
    "# from design.fno_dataset import FNOData\n",
    "from lamp.datasets.load_dataset import load_data\n",
    "from lamp.models import load_model#, rollout, rollout_plasma, plot_vlasov\n",
    "from lamp.pytorch_net.util import Printer, Attr_Dict, get_num_params, get_machine_name, pload, pdump, to_np_array, get_pdict, reshape_weight_to_matrix, ddeepcopy as deepcopy, plot_vectors, record_data, filter_filename, Early_Stopping, str2bool, get_filename_short, print_banner, plot_matrices, get_num_params, init_args, filter_kwargs, to_string, COLOR_LIST\n",
    "# from settings.hyperparams import update_legacy_default_hyperparam\n",
    "# from settings.filepath import EXP_PATH\n",
    "from lamp.utils import LpLoss, to_cpu, to_tuple_shape, parse_multi_step, loss_op, get_cholesky_inverse, get_device, get_data_comb, EXP_PATH, update_legacy_default_hyperparam\n",
    "# from multiscale.utilities import pyg2obj, verface2obj\n",
    "\n",
    "device = torch.device(\"cuda:4\")\n",
    "p = Printer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ca8668-1028-48e9-8e54-edb070617269",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4cfdff-7679-410c-bc38-b8d0e412ab10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting:\n",
    "def plot_learning_curve(data_record):\n",
    "    plt.figure(figsize=(16,6))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(data_record[\"epoch\"], data_record[\"train_loss\"], label=\"train\")\n",
    "    plt.plot(data_record[\"test_epoch\"] if \"test_epoch\" in data_record else data_record[\"epoch\"], data_record[\"val_loss\"], label=\"val\")\n",
    "    plt.plot(data_record[\"test_epoch\"] if \"test_epoch\" in data_record else data_record[\"epoch\"], data_record[\"test_loss\"], label=\"test\")\n",
    "    plt.title(\"Learning curve, linear scale\")\n",
    "    plt.legend()\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.semilogy(data_record[\"epoch\"], data_record[\"train_loss\"], label=\"train\")\n",
    "    plt.semilogy(data_record[\"test_epoch\"] if \"test_epoch\" in data_record else data_record[\"epoch\"], data_record[\"val_loss\"], label=\"val\")\n",
    "    plt.semilogy(data_record[\"test_epoch\"] if \"test_epoch\" in data_record else data_record[\"epoch\"], data_record[\"test_loss\"], label=\"test\")\n",
    "    plt.title(\"Learning curve, log scale\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_colorbar(matrix, vmax=None, vmin=None, cmap=\"seismic\", label=None):\n",
    "    if vmax==None:\n",
    "        vmax = matrix.max()\n",
    "        vmin = matrix.min()\n",
    "    im = plt.imshow(matrix,vmax=vmax,vmin=vmin,cmap=cmap)\n",
    "    plt.title(label)\n",
    "    im_ratio = matrix.shape[0]/matrix.shape[1]\n",
    "    plt.colorbar(im,fraction=0.046*im_ratio,pad=0.04)\n",
    "\n",
    "\n",
    "def visualize(pred, gt, animate=False):\n",
    "    if torch.is_tensor(gt):\n",
    "        gt = to_np_array(gt)\n",
    "        pred = to_np_array(pred)\n",
    "    mse_over_t = ((gt-pred)**2).mean(axis=0).mean(axis=-1)\n",
    "     \n",
    "    if not animate:\n",
    "        vmax = gt.max()\n",
    "        vmin = gt.min()\n",
    "        plt.figure(figsize=[15,5])\n",
    "        plt.subplot(1,4,1)\n",
    "        plot_colorbar(gt[:,:,0].T,label=\"gt\")\n",
    "        plt.subplot(1,4,2)\n",
    "        plot_colorbar(pred[:,:,0].T,label=\"pred\")\n",
    "        plt.subplot(1,4,3)\n",
    "        plot_colorbar((pred-gt)[:,:,0].T,vmax=np.abs(pred-gt).max(),vmin=(-1*np.abs(pred-gt).max()),label=\"diff\")\n",
    "        plt.subplot(1,4,4)\n",
    "        plt.plot(mse_over_t);plt.title(\"mse over t\");plt.yscale('log');\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def visualize_paper(pred, gt, is_save=False):\n",
    "    idx = 6\n",
    "    nx = pred.shape[0]\n",
    "\n",
    "    fontsize = 14\n",
    "    idx_list = np.arange(0, 200, 15)\n",
    "    color_list = np.linspace(0.01, 0.9, len(idx_list))\n",
    "    x_axis = np.linspace(0,16,nx)\n",
    "    cmap = matplotlib.cm.get_cmap('jet')\n",
    "\n",
    "    plt.figure(figsize=(16,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    for i, idx in enumerate(idx_list):\n",
    "        pred_i = to_np_array(pred[...,idx,:].squeeze())\n",
    "        rgb = cmap(color_list[i])[:3]\n",
    "        plt.plot(x_axis, pred_i, color=rgb, label=f\"t={np.round(i*0.3, 1)}s\")\n",
    "    plt.ylabel(\"u(t,x)\", fontsize=fontsize)\n",
    "    plt.xlabel(\"x\", fontsize=fontsize)\n",
    "    plt.tick_params(labelsize=fontsize)\n",
    "    # plt.legend(fontsize=10, bbox_to_anchor=[1,1])\n",
    "    plt.xticks([0,8,16], [0,8,16])\n",
    "    plt.ylim([-2.5,2.5])\n",
    "    plt.title(\"Prediction\")\n",
    "    if is_save:\n",
    "        plt.savefig(f\"1D_E2-{nx}.pdf\", bbox_inches='tight')\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    for i, idx in enumerate(idx_list):\n",
    "        y_i = to_np_array(gt[...,idx,:])\n",
    "        rgb = cmap(color_list[i])[:3]\n",
    "        plt.plot(x_axis, y_i, color=rgb, label=f\"t={np.round(i*0.3, 1)}s\")\n",
    "    plt.ylabel(\"u(t,x)\", fontsize=fontsize)\n",
    "    plt.xlabel(\"x\", fontsize=fontsize)\n",
    "    plt.tick_params(labelsize=fontsize)\n",
    "    plt.legend(fontsize=10, bbox_to_anchor=[1,1])\n",
    "    plt.xticks([0,8,16], [0,8,16])\n",
    "    plt.ylim([-2.5,2.5])\n",
    "    plt.title(\"Ground-truth\")\n",
    "    if is_save:\n",
    "        plt.savefig(f\"1D_gt-{nx}.pdf\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "def yield_f_file(path, filename):\n",
    "    \"\"\" Generate data of faces.\n",
    "    Args:\n",
    "        path: path to the folder in which folder is.\n",
    "        filename: name of the obj filename.\n",
    "\n",
    "    Returns:\n",
    "        list of faces.        \n",
    "    \"\"\"\n",
    "    f = open(os.path.join(path, filename))\n",
    "    buf = f.read()\n",
    "    f.close()\n",
    "    for b in buf.split('\\n'):\n",
    "        if b.startswith('f '):\n",
    "            triangles = b.split(' ')[1:]\n",
    "            # -1 as .obj is base 1 but the Data class expects base 0 indices\n",
    "            yield [int(t.split(\"/\")[0]) - 1 for t in triangles]\n",
    "            \n",
    "def gen_v_file(path, filename):\n",
    "    \"\"\" Generate data of vertices.\n",
    "    Args:\n",
    "        path: path to the folder in which folder is.\n",
    "        filename: name of the obj filename.\n",
    "\n",
    "    Returns:\n",
    "        list of vertices. Each vertex is represented by 3d vector.       \n",
    "    \"\"\"\n",
    "    f = open(os.path.join(path, filename))\n",
    "    buf = f.read()\n",
    "    f.close()\n",
    "    nodes = []\n",
    "    for b in buf.split('\\n'):\n",
    "        if b.startswith('v '):\n",
    "            nodes.append([float(x) for x in b.split(\" \")[1:]])\n",
    "    return np.array(nodes)\n",
    "\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "def to_device_recur(iterable, device, is_detach=False):\n",
    "    if isinstance(iterable, list):\n",
    "        return [to_device_recur(item, device, is_detach=is_detach) for item in iterable]\n",
    "    elif isinstance(iterable, tuple):\n",
    "        return tuple(to_device_recur(item, device, is_detach=is_detach) for item in iterable)\n",
    "    elif isinstance(iterable, dict):\n",
    "        Dict = {key: to_device_recur(item, device, is_detach=is_detach) for key, item in iterable.items()}\n",
    "        if iterable.__class__.__name__ == \"Pdict\":\n",
    "            from pstar import pdict\n",
    "            class Pdict(pdict):\n",
    "                def to(self, device):\n",
    "                    self[\"device\"] = device\n",
    "                    return to_device_recur(self, device)\n",
    "\n",
    "                def copy(self):\n",
    "                    return Pdict(dict.copy(self))\n",
    "            Dict = Pdict(Dict)\n",
    "        elif iterable.__class__.__name__ == \"Attr_Dict\":\n",
    "            Dict = Attr_Dict(Dict)\n",
    "        return Dict\n",
    "    elif hasattr(iterable, \"to\"):\n",
    "        iterable = iterable.to(device)\n",
    "        if is_detach:\n",
    "            iterable = iterable.detach()\n",
    "        return iterable\n",
    "    else:\n",
    "        if hasattr(iterable, \"detach\"):\n",
    "            iterable = iterable.detach()\n",
    "        return iterable\n",
    "\n",
    "def pyg_to_dolfin_mesh(vers, faces):\n",
    "    mesh = dolfin.Mesh()\n",
    "    editor = dolfin.MeshEditor()\n",
    "    editor.open(mesh, 'triangle', 2, 2)\n",
    "\n",
    "    editor.init_vertices(vers.shape[0])\n",
    "    for i in range(vers.shape[0]):\n",
    "        editor.add_vertex(i, vers[i,:2])\n",
    "    editor.init_cells(faces.shape[0])\n",
    "    for f in range(faces.shape[0]):\n",
    "        editor.add_cell(f, faces[f])\n",
    "\n",
    "    editor.close()\n",
    "    return mesh\n",
    "\n",
    "def generate_baryweight(tarvers, mesh, bvh_tree):\n",
    "    faces = []\n",
    "    weights = []\n",
    "    for query in tarvers:\n",
    "        face = bvh_tree.compute_first_entity_collision(dolfin.Point(query))\n",
    "        while (mesh.num_cells() <= face):\n",
    "            #print(\"query: \", query)\n",
    "            if query[0] < 0.5:\n",
    "                query[0] += 1e-15\n",
    "            elif query[0] >= 0.5:\n",
    "                query[0] -= 1e-15\n",
    "            if query[1] < 0.5:\n",
    "                query[1] += 1e-15\n",
    "            elif query[1] >= 0.5:\n",
    "                query[1] -= 1e-15            \n",
    "            face = bvh_tree.compute_first_entity_collision(dolfin.Point(query))\n",
    "        faces.append(face)\n",
    "        face_coords = mesh.coordinates()[mesh.cells()[face]]\n",
    "        mat = face_coords.T[:,[0,1]] - face_coords.T[:,[2,2]]\n",
    "        const = query - face_coords[2,:]\n",
    "        weight = np.linalg.solve(mat, const)\n",
    "        final_weights = np.concatenate([weight, np.ones(1) - weight.sum()], axis=-1)\n",
    "        weights.append(final_weights)\n",
    "    return faces, weights\n",
    "\n",
    "def generate_barycentric_interpolated_data(mesh, bvhtree, outvec, tarvers):\n",
    "    faces, weights = generate_baryweight(tarvers, mesh, bvhtree)\n",
    "    indices = mesh.cells()[faces].astype('int64')\n",
    "    fweights = torch.tensor(np.array(weights), dtype=torch.float32)\n",
    "    return torch.matmul(fweights, torch.tensor(outvec[indices,:], dtype=torch.float32)).diagonal().T\n",
    "    \n",
    "# Analysis:\n",
    "def get_results_2d(all_hash, mode=\"best\", dirname=None, suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Perform analysis on the 1D Burgers' benchmark.\n",
    "\n",
    "    Args:\n",
    "        all_hash: a list of hashes which indicates the experiments to load for analysis\n",
    "        mode: choose from \"best\" (load the best model with lowest validation loss) or an integer, \n",
    "            e.g. -1 (last saved model), -2 (second last saved model)\n",
    "        dirname: if not None, will use the dirnaem provided. E.g. tailin-1d_2022-7-27\n",
    "        suffix: suffix for saving the analysis result.\n",
    "    \"\"\"\n",
    "    if not dirname.endswith(\"/\"):\n",
    "        dirname += \"/\"\n",
    "    all_dict = {}\n",
    "    isplot = True\n",
    "    for hash_str in all_hash:\n",
    "        # Load model:\n",
    "        filename = filter_filename(dirname, include=hash_str)\n",
    "        try:\n",
    "            data_record = pload(dirname + filename[0])\n",
    "        except Exception as e:\n",
    "            print(f\"error {e} in hash_str {hash_str}\")\n",
    "            continue\n",
    "        p.print(f\"Hash {hash_str}, best model at epoch {data_record['best_epoch']}:\", banner_size=160)\n",
    "        if isplot:\n",
    "            plot_learning_curve(data_record)\n",
    "        args = init_args(update_legacy_default_hyperparam(data_record[\"args\"]))\n",
    "        args.filename = filename\n",
    "        data_record['best_model_dict']['type'] = 'GNNRemesherPolicy'\n",
    "        # data_record['best_model_dict']['edge_threshold'] = 0.\n",
    "        data_record['best_model_dict']['noise_amp'] = 0.\n",
    "        data_record['best_model_dict'][\"correction_rate\"] = 0.\n",
    "        data_record['best_model_dict'][\"batch_size\"] = 16\n",
    "        if mode == \"best\":\n",
    "            model = load_model(data_record[\"best_model_dict\"], device=device)\n",
    "            print(\"Load the model with best validation loss.\")\n",
    "        else:\n",
    "            assert isinstance(mode, int)\n",
    "            print(f'Load the model at epoch {data_record[\"epoch\"][mode]}')\n",
    "            model = load_model(data_record[\"model_dict\"][mode], device=device)\n",
    "        model.eval()\n",
    "\n",
    "        # Load test dataset:\n",
    "        args_test = deepcopy(args)\n",
    "        #multi_step = (326 - 121) // args_test.temporal_bundle_steps\n",
    "        multi_step = 125\n",
    "        # multi_step = 250\n",
    "        args_test.multi_step = f\"1^{multi_step}\"\n",
    "        args_test.is_test_only = True\n",
    "        args_test.is_shifted_data = True\n",
    "        args_test.use_fineres_data=True\n",
    "        args_test.is_1d_periodic=False\n",
    "        args_test.is_normalize_pos=False\n",
    "        n_test_traj = 50\n",
    "\n",
    "        (dataset_train_val, dataset_test), (train_loader, val_loader, test_loader) = load_data(args_test)\n",
    "\n",
    "        loss_list = []\n",
    "        pred_list = []\n",
    "        predl2_list = []\n",
    "        y_list = []\n",
    "        time_stamps_effective = len(dataset_test) // n_test_traj\n",
    "        kwargs = {}\n",
    "        # input_margin = 40\n",
    "        input_margin = 0\n",
    "        total_instances = 0\n",
    "        spatial_loss = []\n",
    "        for i in range(0,n_test_traj):\n",
    "            idx = i * time_stamps_effective + input_margin\n",
    "            data = dataset_test[idx]\n",
    "            data.batch_history = {\"n0\": torch.empty([1], device=device)}\n",
    "            data = data.to(device)\n",
    "            _, info = model.interpolation_hist_forward(\n",
    "                data,\n",
    "                pred_steps=multi_step-2*input_margin-1,\n",
    "                train=False,\n",
    "                use_pos=False,\n",
    "                **kwargs\n",
    "            )            \n",
    "            preds = [p.detach().cpu() for p in info[\"data_x\"]]\n",
    "            faces = [f.detach().cpu().numpy().T for f in info[\"data_face\"]]\n",
    "            ylabels = [y.detach().cpu().numpy() for y in list(info[\"data\"][0].yfeatures)]\n",
    "            info[\"data\"][-1].x = info[\"final_gnnout\"].detach().cpu()\n",
    "            info[\"data\"][-1].xfaces = torch.tensor(info[\"data\"][-1].yface_list[multi_step-2*input_margin-1-2]).T\n",
    "            temp_loss_list = []\n",
    "            temp_spatial_loss = []\n",
    "            temp_lossl2_list = []\n",
    "            for j in range(len(info[\"data_x\"])):\n",
    "                bary_coords = preds[j].numpy()\n",
    "                bary_faces = faces[j]\n",
    "                targets = ylabels[j+1][:,:2]\n",
    "                mesh = pyg_to_dolfin_mesh(bary_coords[:,:2], bary_faces)\n",
    "                bvhtree = mesh.bounding_box_tree() \n",
    "                interp_nodes = generate_barycentric_interpolated_data(mesh, bvhtree, bary_coords, targets)\n",
    "                tlossl2 = ((interp_nodes[:,3:] - torch.tensor(ylabels[j+1][:,3:]))**2).sum(dim=-1).sqrt().sum()/ylabels[j+1].shape[0]\n",
    "                tloss = (nn.MSELoss(reduction=\"sum\")(interp_nodes[:,3:], torch.tensor(ylabels[j+1][:,3:], dtype=torch.float32))/ylabels[j+1].shape[0])# .detach().cpu()\n",
    "                temp_loss_list.append(tloss)\n",
    "                temp_lossl2_list.append(tlossl2)\n",
    "                spaceloss = (nn.MSELoss(reduction=\"sum\")(interp_nodes[:,3:], torch.tensor(ylabels[j+1][:,3:], dtype=torch.float32))) #/preds[j].shape[0])\n",
    "                temp_spatial_loss.append(spaceloss)\n",
    "                total_instances += info[\"data_x\"][j].shape[0]\n",
    "            loss_list.append(sum(temp_loss_list).item())\n",
    "            spatial_loss.append(sum(temp_spatial_loss).item())\n",
    "            pred_list.append(temp_loss_list)\n",
    "            predl2_list.append(temp_lossl2_list)\n",
    "            \n",
    "            break\n",
    "        loss_mean = np.mean(loss_list)\n",
    "        space_rmse = math.sqrt(sum(spatial_loss)/total_instances)\n",
    "        all_dict[hash_str] = (data_record['best_epoch'], loss_mean, len(data_record[\"train_loss\"]), args.epochs)\n",
    "        print(all_dict[hash_str])\n",
    "        print(\"Rollout RMSE over all spatial coordinates, all mesh nodes, all time step, and all trajecoties for {} is {:.6e}\".format(hash_str, space_rmse))\n",
    "        print(\"Test for {} is: {:.6e} at epoch {}\".format(hash_str, loss_mean, data_record['best_epoch']))\n",
    "\n",
    "        mse_full = torch.tensor(pred_list) # nn.MSELoss(reduction=\"none\")(pred_list, y_list)\n",
    "        mse_time = to_np_array(mse_full.mean((0), True))[0,:]\n",
    "        p.print(\"Learning curve:\", is_datetime=False, banner_size=100)\n",
    "        plt.figure(figsize=(12,5))\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot(mse_time)\n",
    "        plt.xlabel(\"rollout step\")\n",
    "        plt.ylabel(\"MSE\")\n",
    "        plt.title(\"MSE vs. rollout step (linear scale)\")\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.semilogy(mse_time)\n",
    "        plt.xlabel(\"rollout step\")\n",
    "        plt.ylabel(\"MSE\")\n",
    "        plt.title(\"MSE vs. rollout step (log scale)\")\n",
    "        plt.show()\n",
    "        plt.figure(figsize=(6,5))\n",
    "        plt.plot(mse_time.cumsum())\n",
    "        plt.title(\"cumulative MSE vs. rollout step\")\n",
    "        plt.xlabel(\"rollout step\")\n",
    "        plt.ylabel(\"cumulative MSE\")\n",
    "        plt.show()\n",
    "        \n",
    "        #break\n",
    "\n",
    "    pdump(all_dict, f\"all_dict_1d{suffix}.p\")\n",
    "    return info, [y.detach().cpu().numpy() for y in list(info[\"data\"][0].y_tar)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa49119-ad2f-4725-897d-d32632c47e3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2023-02-18\n",
    "is_visualization = False\n",
    "dirname = EXP_PATH + \"evo-2d_2023-02-18\"\n",
    "new_hash = [\n",
    "    \"jNdZioUu_ampere2\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dbf8e0-24ce-4f47-bdf5-ece5f354baaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for nhash in new_hash:\n",
    "    info, ylabels = get_results_2d([nhash], dirname=dirname)\n",
    "    hashstr = nhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af04aced-537d-41f9-b6cb-5b4dae325929",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lamp2",
   "language": "python",
   "name": "lamp2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
