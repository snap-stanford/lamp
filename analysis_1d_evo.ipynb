{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca791c51-0ee5-40b0-b207-75ac8e2a491d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import argparse\n",
    "from collections import OrderedDict\n",
    "import datetime\n",
    "import gc\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "from numbers import Number\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 1500\n",
    "pd.options.display.max_columns = 200\n",
    "pd.options.display.width = 1000\n",
    "pd.set_option('max_colwidth', 400)\n",
    "import pdb\n",
    "import pickle\n",
    "import pprint as pp\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from deepsnap.batch import Batch as deepsnap_Batch\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..'))\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..', '..'))\n",
    "from lamp.argparser import arg_parse\n",
    "from lamp.datasets.load_dataset import load_data\n",
    "from lamp.gnns import get_data_dropout\n",
    "from lamp.models import load_model\n",
    "from lamp.pytorch_net.util import groupby_add_keys, filter_df, get_unique_keys_df, Attr_Dict, Printer, get_num_params, get_machine_name, pload, pdump, to_np_array, get_pdict, reshape_weight_to_matrix, ddeepcopy as deepcopy, plot_vectors, record_data, filter_filename, Early_Stopping, str2bool, get_filename_short, print_banner, plot_matrices, get_num_params, init_args, filter_kwargs, to_string, COLOR_LIST\n",
    "from lamp.utils import p, update_legacy_default_hyperparam, EXP_PATH, deepsnap_to_pyg, LpLoss, to_cpu, to_tuple_shape, parse_multi_step, loss_op, get_cholesky_inverse, get_device, get_data_comb\n",
    "\n",
    "device = torch.device(\"cuda:5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd583f18-3ce5-469a-a89f-a19887163e00",
   "metadata": {},
   "source": [
    "## Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5ff29f-9493-4e3b-a5e7-9545e7c0a143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis:\n",
    "def get_results_1d(\n",
    "    all_hash,\n",
    "    mode=\"best\",\n",
    "    exclude_idx=(None,),\n",
    "    dropout_mode=\"None\",\n",
    "    n_rollout_steps=-1,\n",
    "    dirname=None,\n",
    "    suffix=\"\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform analysis on the 1D Burgers' benchmark.\n",
    "\n",
    "    Args:\n",
    "        all_hash: a list of hashes which indicates the experiments to load for analysis\n",
    "        mode: choose from \"best\" (load the best model with lowest validation loss) or an integer, \n",
    "            e.g. -1 (last saved model), -2 (second last saved model)\n",
    "        dirname: if not None, will use the dirnaem provided. E.g. tailin-1d_2022-7-27\n",
    "        suffix: suffix for saving the analysis result.\n",
    "    \"\"\"\n",
    "    \n",
    "    isplot = True\n",
    "    df_dict_list = []\n",
    "    dirname_start = dirname\n",
    "    for hash_str in all_hash:\n",
    "        df_dict = {}\n",
    "        df_dict[\"hash\"] = hash_str\n",
    "        # Load model:\n",
    "        is_found = False\n",
    "        for dirname_core in [\n",
    "             dirname_start,\n",
    "            ]:\n",
    "            filename = filter_filename(EXP_PATH + dirname_core, include=hash_str)\n",
    "            if len(filename) == 1:\n",
    "                is_found = True\n",
    "                break\n",
    "        if not is_found:\n",
    "            print(f\"hash {hash_str} does not exist in {dirname}! Please pass in the correct dirname.\")\n",
    "            continue\n",
    "        dirname = EXP_PATH + dirname_core\n",
    "        if not dirname.endswith(\"/\"):\n",
    "            dirname += \"/\"\n",
    "\n",
    "        try:\n",
    "            data_record = pload(dirname + filename[0])\n",
    "        except Exception as e:\n",
    "            # p.print(f\"Hash {hash_str}, best model at epoch {data_record['best_epoch']}:\", banner_size=100)\n",
    "            print(f\"error {e} in hash_str {hash_str}\")\n",
    "            continue\n",
    "        p.print(f\"Hash {hash_str}, best model at epoch {data_record['best_epoch']}:\", banner_size=160)\n",
    "        if isplot:\n",
    "            plot_learning_curve(data_record)\n",
    "        args = init_args(update_legacy_default_hyperparam(data_record[\"args\"]))\n",
    "        args.filename = filename\n",
    "        if mode == \"best\":\n",
    "            model = load_model(data_record[\"best_model_dict\"], device=device)\n",
    "            print(\"Load the model with best validation loss.\")\n",
    "        else:\n",
    "            assert isinstance(mode, int)\n",
    "            print(f'Load the model at epoch {data_record[\"epoch\"][mode]}')\n",
    "            model = load_model(data_record[\"model_dict\"][mode], device=device)\n",
    "        model.eval()\n",
    "        # pp.pprint(args.__dict__)\n",
    "        kwargs = {}\n",
    "        if data_record[\"best_model_dict\"][\"type\"].startswith(\"GNNPolicy\"):\n",
    "            kwargs[\"is_deepsnap\"] = True\n",
    "\n",
    "        # Load test dataset:\n",
    "        args_test = deepcopy(args)\n",
    "        multi_step = (250 - 50) // args_test.temporal_bundle_steps\n",
    "        args_test.multi_step = f\"1^{multi_step}\"\n",
    "        args_test.is_test_only = True\n",
    "        args_test.n_train = \"-1\"\n",
    "        n_test_traj = 128\n",
    "        (dataset_train_val, dataset_test), (train_loader, val_loader, test_loader) = load_data(args_test)\n",
    "        nx = int(args.dataset.split(\"-\")[2])\n",
    "        time_stamps_effective = len(dataset_test) // n_test_traj\n",
    "        for exclude_idx_ele in exclude_idx:\n",
    "            loss_list = []\n",
    "            pred_list = []\n",
    "            y_list = []\n",
    "            for i in range(n_test_traj):\n",
    "                idx = i * time_stamps_effective + args_test.temporal_bundle_steps\n",
    "                data = deepcopy(dataset_test[idx])\n",
    "                if dropout_mode == \"None\":\n",
    "                    if exclude_idx_ele is not None:\n",
    "                        data = get_data_dropout(data, dropout_mode=\"node:0\", exclude_idx=exclude_idx_ele)\n",
    "                else:\n",
    "                    data = get_data_dropout(data, dropout_mode=dropout_mode)\n",
    "                data = data.to(device)\n",
    "                preds, info = model(\n",
    "                    data,\n",
    "                    pred_steps=np.arange(1,n_rollout_steps+1) if n_rollout_steps != -1 else np.arange(1, max(parse_multi_step(args_test.multi_step).keys())+1),\n",
    "                    latent_pred_steps=None,\n",
    "                    is_recons=False,\n",
    "                    use_grads=False,\n",
    "                    use_pos=args.use_pos,\n",
    "                    is_y_diff=False,\n",
    "                    is_rollout=False,\n",
    "                    **kwargs\n",
    "                )\n",
    "                y = data.node_label[\"n0\"]\n",
    "                if n_rollout_steps != -1:\n",
    "                    y = y[:,:25*n_rollout_steps]\n",
    "                pred = preds[\"n0\"].reshape(y.shape)\n",
    "                pred_list.append(pred.detach())\n",
    "                y_list.append(y.detach())\n",
    "                loss_ele = nn.MSELoss(reduction=\"sum\")(pred, y) / nx\n",
    "                loss_list.append(loss_ele.item())\n",
    "\n",
    "            loss_mean = np.mean(loss_list)\n",
    "            pred_list = torch.stack(pred_list).squeeze(-1)\n",
    "            y_list = torch.stack(y_list).squeeze(-1)\n",
    "            df_dict[f\"loss_cumu_{exclude_idx_ele}\"] = loss_mean \n",
    "            print(\"\\nTest for {} for exclude_idx={} is: {:.9f} at epoch {}, for {}/{} epochs\".format(hash_str, exclude_idx_ele, loss_mean, data_record['best_epoch'], len(data_record[\"train_loss\"]), args.epochs))\n",
    "\n",
    "            mse_full = nn.MSELoss(reduction=\"none\")(pred_list, y_list)\n",
    "            mse_time = to_np_array(mse_full.mean((0,1)))\n",
    "            p.print(\"Learning curve:\", is_datetime=False, banner_size=100)\n",
    "            plt.figure(figsize=(12,5))\n",
    "            plt.subplot(1,2,1)\n",
    "            plt.plot(mse_time)\n",
    "            plt.xlabel(\"rollout step\")\n",
    "            plt.ylabel(\"MSE\")\n",
    "            plt.title(\"MSE vs. rollout step (linear scale)\")\n",
    "            plt.subplot(1,2,2)\n",
    "            plt.semilogy(mse_time)\n",
    "            plt.xlabel(\"rollout step\")\n",
    "            plt.ylabel(\"MSE\")\n",
    "            plt.title(\"MSE vs. rollout step (log scale)\")\n",
    "            plt.show()\n",
    "            plt.figure(figsize=(6,5))\n",
    "            plt.plot(mse_time.cumsum())\n",
    "            plt.title(\"cumulative MSE vs. rollout step\")\n",
    "            plt.xlabel(\"rollout step\")\n",
    "            plt.ylabel(\"cumulative MSE\")\n",
    "            plt.show()\n",
    "\n",
    "            # Visualization:\n",
    "            for idx in range(6,8):\n",
    "                p.print(f\"Example {idx*128}:\", banner_size=100, is_datetime=False)\n",
    "                data = deepcopy(dataset_test[idx*128]).to(device)\n",
    "                if exclude_idx_ele is not None:\n",
    "                    data = get_data_dropout(data, dropout_mode=\"node:0\", exclude_idx=exclude_idx_ele)\n",
    "                preds, info = model(\n",
    "                    data,\n",
    "                    pred_steps=np.arange(1,max(parse_multi_step(args_test.multi_step).keys())+1),\n",
    "                    latent_pred_steps=None,\n",
    "                    is_recons=False,\n",
    "                    use_grads=False,\n",
    "                    use_pos=args.use_pos,\n",
    "                    is_y_diff=False,\n",
    "                    is_rollout=False,\n",
    "                    **kwargs\n",
    "                )\n",
    "                y = data.node_label[\"n0\"]\n",
    "                pred = preds[\"n0\"].reshape(y.shape)\n",
    "                visualize(pred, y)\n",
    "                visualize_paper(pred, y)\n",
    "\n",
    "            p.print(f\"Individual prediction at rollout step {y.shape[1]}:\", banner_size=100, is_datetime=False)\n",
    "            time_step = -1\n",
    "            for idx in range(0, 20, 5):\n",
    "                plt.figure(figsize=(6,4))\n",
    "                plt.plot(to_np_array(pred_list[idx,:,time_step]), label=\"pred\")\n",
    "                plt.plot(to_np_array(y_list[idx,:,time_step]), \"--\", label=\"y\")\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "        df_dict[\"best_epoch\"] = data_record['best_epoch']\n",
    "        df_dict[\"epoch\"] = len(data_record[\"train_loss\"])\n",
    "        df_dict.update(args.__dict__)\n",
    "        df_dict_list.append(df_dict)\n",
    "    df = pd.DataFrame(df_dict_list)\n",
    "    pdump(df, f\"df_1d{suffix}.p\")\n",
    "    return df\n",
    "\n",
    "# Plotting:\n",
    "def plot_learning_curve(data_record):\n",
    "    plt.figure(figsize=(16,6))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(data_record[\"epoch\"], data_record[\"train_loss\"], label=\"train\")\n",
    "    plt.plot(data_record[\"test_epoch\"] if \"test_epoch\" in data_record else data_record[\"epoch\"], data_record[\"val_loss\"], label=\"val\")\n",
    "    plt.plot(data_record[\"test_epoch\"] if \"test_epoch\" in data_record else data_record[\"epoch\"], data_record[\"test_loss\"], label=\"test\")\n",
    "    plt.title(\"Learning curve, linear scale\")\n",
    "    plt.legend()\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.semilogy(data_record[\"epoch\"], data_record[\"train_loss\"], label=\"train\")\n",
    "    plt.semilogy(data_record[\"test_epoch\"] if \"test_epoch\" in data_record else data_record[\"epoch\"], data_record[\"val_loss\"], label=\"val\")\n",
    "    plt.semilogy(data_record[\"test_epoch\"] if \"test_epoch\" in data_record else data_record[\"epoch\"], data_record[\"test_loss\"], label=\"test\")\n",
    "    plt.title(\"Learning curve, log scale\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_colorbar(matrix, vmax=None, vmin=None, cmap=\"seismic\", label=None):\n",
    "    if vmax==None:\n",
    "        vmax = matrix.max()\n",
    "        vmin = matrix.min()\n",
    "    im = plt.imshow(matrix,vmax=vmax,vmin=vmin,cmap=cmap)\n",
    "    plt.title(label)\n",
    "    im_ratio = matrix.shape[0]/matrix.shape[1]\n",
    "    plt.colorbar(im,fraction=0.046*im_ratio,pad=0.04)\n",
    "\n",
    "\n",
    "def visualize(pred, gt, animate=False):\n",
    "    if torch.is_tensor(gt):\n",
    "        gt = to_np_array(gt)\n",
    "        pred = to_np_array(pred)\n",
    "    mse_over_t = ((gt-pred)**2).mean(axis=0).mean(axis=-1)\n",
    "     \n",
    "    if not animate:\n",
    "        vmax = gt.max()\n",
    "        vmin = gt.min()\n",
    "        plt.figure(figsize=[15,5])\n",
    "        plt.subplot(1,4,1)\n",
    "        plot_colorbar(gt[:,:,0].T,label=\"gt\")\n",
    "        plt.subplot(1,4,2)\n",
    "        plot_colorbar(pred[:,:,0].T,label=\"pred\")\n",
    "        plt.subplot(1,4,3)\n",
    "        plot_colorbar((pred-gt)[:,:,0].T,vmax=np.abs(pred-gt).max(),vmin=(-1*np.abs(pred-gt).max()),label=\"diff\")\n",
    "        plt.subplot(1,4,4)\n",
    "        plt.plot(mse_over_t);plt.title(\"mse over t\");plt.yscale('log');\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def visualize_paper(pred, gt, is_save=False):\n",
    "    idx = 6\n",
    "    nx = pred.shape[0]\n",
    "\n",
    "    fontsize = 14\n",
    "    idx_list = np.arange(0, 200, 15)\n",
    "    color_list = np.linspace(0.01, 0.9, len(idx_list))\n",
    "    x_axis = np.linspace(0,16,nx)\n",
    "    cmap = matplotlib.cm.get_cmap('jet')\n",
    "\n",
    "    plt.figure(figsize=(16,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    for i, idx in enumerate(idx_list):\n",
    "        pred_i = to_np_array(pred[...,idx,:].squeeze())\n",
    "        rgb = cmap(color_list[i])[:3]\n",
    "        plt.plot(x_axis, pred_i, color=rgb, label=f\"t={np.round(i*0.3, 1)}s\")\n",
    "    plt.ylabel(\"u(t,x)\", fontsize=fontsize)\n",
    "    plt.xlabel(\"x\", fontsize=fontsize)\n",
    "    plt.tick_params(labelsize=fontsize)\n",
    "    # plt.legend(fontsize=10, bbox_to_anchor=[1,1])\n",
    "    plt.xticks([0,8,16], [0,8,16])\n",
    "    plt.ylim([-2.5,2.5])\n",
    "    plt.title(\"Prediction\")\n",
    "    if is_save:\n",
    "        plt.savefig(f\"1D_E2-{nx}.pdf\", bbox_inches='tight')\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    for i, idx in enumerate(idx_list):\n",
    "        y_i = to_np_array(gt[...,idx,:])\n",
    "        rgb = cmap(color_list[i])[:3]\n",
    "        plt.plot(x_axis, y_i, color=rgb, label=f\"t={np.round(i*0.3, 1)}s\")\n",
    "    plt.ylabel(\"u(t,x)\", fontsize=fontsize)\n",
    "    plt.xlabel(\"x\", fontsize=fontsize)\n",
    "    plt.tick_params(labelsize=fontsize)\n",
    "    plt.legend(fontsize=10, bbox_to_anchor=[1,1])\n",
    "    plt.xticks([0,8,16], [0,8,16])\n",
    "    plt.ylim([-2.5,2.5])\n",
    "    plt.title(\"Ground-truth\")\n",
    "    if is_save:\n",
    "        plt.savefig(f\"1D_gt-{nx}.pdf\", bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc711711-1ca8-4944-92d3-848313c275e9",
   "metadata": {},
   "source": [
    "## Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b58196-d00d-4519-8e3b-a0789dbf9444",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "all_hash is a list of hashes, each of which corresponds to one experiment.\n",
    "    For example, if one experiment is saved under ./results/evo-1d_2023-01-01/mppde1d-E2-50_train_-1_algo_contrast_ebm_False_ebmt_cd_enc_cnn-s_evo_cnn_act_elu_hid_128_lo_rmse_recef_1.0_conef_1.0_nconv_4_nlat_1_clat_3_lf_True_reg_None_id_0_Hash_qvQry9QJ_ampere3.p\n",
    "    Then, the \"qvQry9QJ\" (located at the end of the filename) is the {hash} of this file.\n",
    "    The \"evo-1d_2023-01-01\" is the \"{--exp_id}_{--date_time}\" of the training command.\n",
    "    all_hash can contain multiple hashes, and analyze them sequentially.\n",
    "\"\"\"\n",
    "all_hash = [\n",
    "    \"mhkVkAaz\",\n",
    "]\n",
    "df9 = get_results_1d(\n",
    "    all_hash,\n",
    "    dirname=\"evo-1d_2023-01-01\",\n",
    "    n_rollout_steps=7,\n",
    "    dropout_mode=\"uniform:2\",\n",
    "    suffix=\"_0\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
